# -*- coding: utf-8 -*-
"""WS5 Edited!!!

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_53t8gcSc50M4_Y80LhgYquAv337WWet
"""

!pip install shap -q


import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, train_test_split


from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import ExtraTreesRegressor
import joblib
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import matplotlib as mpl
import scipy.stats as stats
import plotly.graph_objects as go
import plotly.express as px
from scipy.stats import spearmanr
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform

# Import filters to remove unnecessary warnings
from warnings import simplefilter
import warnings
warnings.filterwarnings("ignore")
from sklearn.exceptions import ConvergenceWarning

from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform

# Import filters to remove unnecessary warnings
from warnings import simplefilter
import warnings
warnings.filterwarnings("ignore")

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error
import shap

import pandas as pd
from google.colab import files

#upload the file
uploaded = files.upload()

#load the dataset
file_name = "Hydrogen storage dataset.xlsx"
df = pd.read_excel(file_name, header = 2)

df.head()

# check if the data contains null values
df.isna().sum()

# Check for duplicates in the entire dataset
duplicates = df.duplicated()
# If there are any duplicates, the 'duplicates' variable will contain True for those rows
if duplicates.any():
    # Get the rows with duplicates
    duplicate_rows = df[duplicates]
else:
    print("No duplicates found in the dataset.")

# check for data types
df.dtypes

# Summary statistics for numerical columns
df.describe()

df['Pressure (bar)'] = df['Pressure (bar)'].astype(float)

# Identify numeric columns
numeric_columns = df.select_dtypes(include=['float', 'int']).columns

# Identify categorical columns (assuming they are of type 'object')
categorical_columns = df.select_dtypes(include=['object']).columns

#Apply one-hot encoding to categorical variables(Catalyst)
#Often used to convert categorical data directly into numerical form for machine learning
df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)

#Display the first few rows of the enncoded dataset
df_encoded.head()

# visualization
sns.pairplot(df)

features = df.columns[1:-3]
targets = df.columns[-3:]

# Create a correlation heatmap for numeric columns
numeric_df = df.select_dtypes(include=[np.number])
plt.figure(figsize=(12, 10))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Heatmap of Numeric Features and Targets')
plt.show()

# Create scatter plots for each numeric feature against each target
numeric_features = df[features].select_dtypes(include=[np.number]).columns
n_features = len(numeric_features)
n_targets = len(targets)
n_cols = n_targets
n_rows = n_features

fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
fig.suptitle('Scatter Plots: Numeric Features vs Targets', fontsize=16)

for i, feature in enumerate(numeric_features):
    for j, target in enumerate(targets):
        ax = axes[i, j]
        ax.scatter(df[feature], df[target], alpha=0.5)
        ax.set_xlabel(feature)
        ax.set_ylabel(target)
        ax.set_title(f'{feature} vs {target}')

plt.tight_layout()
plt.show()

# Create pairwise plots for numeric features
sns.pairplot(df[list(numeric_features) + list(targets)], height=2.5)
plt.suptitle('Pairwise Plots of Numeric Features and Targets', y=1.02)
plt.show()

df['Hydrogen uptake/adsorption (wt%)'].fillna(df['Hydrogen uptake/adsorption (wt%)'].mean(), inplace=True)
df['Hydrogen desorption (wt%)'].fillna(df['Hydrogen desorption (wt%)'].mean(), inplace=True)

# Analyze the correlations between features and target variables

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select only numeric columns from the DataFrame
numeric_df = df.select_dtypes(include=[float, int])

# Calculate the correlation matrix
corr_matrix = numeric_df.corr()

# Print the correlation matrix
print("Correlation Matrix:\n", corr_matrix)

# Identify pairs of highly correlated variables
threshold = 0.9
high_corr_pairs = []

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

print("\nHighly Correlated Pairs (threshold = 0.9):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]}: {pair[2]:.2f}")

#Combining highly correlated variables
# Standardize the data
numeric_df = df.select_dtypes(include=[float, int])
scaler = StandardScaler()

# Drop rows with NaN values before scaling
numeric_df = numeric_df.dropna() # Added line to drop rows with NaN values
scaled_data = scaler.fit_transform(numeric_df)

# Apply PCA to reduce multicollinearity
pca = PCA(n_components=len(numeric_df.columns))
pca.fit(scaled_data)

# Check explained variance to decide how many components to keep
explained_variance = pca.explained_variance_ratio_
print("Explained variance by each principal component:", explained_variance)

cumulative_variance = explained_variance.cumsum()
n_components_to_keep = next(i for i, total_var in enumerate(cumulative_variance) if total_var > 0.95) + 1

# Transform data using the chosen number of components
df_pca = pca.transform(scaled_data)[:, :n_components_to_keep]

print(f"Reduced data using {n_components_to_keep} principal components.")

numerical_columns = df.select_dtypes(include=['float64', 'int64'])  # Selecting only numerical columns
correlations = numerical_columns.corr()['Hydrogen uptake/adsorption (wt%)']  # Computing the correlation matrix
print(correlations)

numerical_columns = df.select_dtypes(include=['float64', 'int64'])  # Selecting only numerical columns
correlations = numerical_columns.corr()['Hydrogen desorption (wt%)']  # Computing the correlation matrix
print(correlations)

df['Material name'].value_counts() # number of occurence of each unique value

# Data Cleansing
from sklearn.model_selection import train_test_split

X = df[['Pressure (bar)']]
y = df['Hydrogen desorption (wt%)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()
linear_regression.fit(X = X_train, y = y_train)

print('β1 = ' + str(linear_regression.coef_) + ', β0 = ' + str(linear_regression.intercept_))

from sklearn.metrics import r2_score
y_pred_test = linear_regression.predict(X_test)
y_pred_train = linear_regression.predict(X_train)

print('R2 train = ', r2_score(y_train, y_pred_train))
print('R2 test = ', r2_score(y_test, y_pred_test))

plt.scatter(y_train,y_pred_train, label='Training Set')
plt.scatter(y_test,y_pred_test, label='Test Set')
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split

X = df.drop(['Material name', 'Hydrogen desorption (wt%)'], axis=1)
y = df['Hydrogen desorption (wt%)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.linear_model import LinearRegression

multiple_linear_regression = LinearRegression()
multiple_linear_regression.fit(X = X_train, y = y_train)

from sklearn.metrics import r2_score

pred_train_lr = multiple_linear_regression.predict(X_train)
pred_test_lr = multiple_linear_regression.predict(X_test)

print('R2 training = ', r2_score(y_train, pred_train_lr))
print('R2 test = ', r2_score(y_test, pred_test_lr))

from sklearn.metrics import mean_squared_error
rmse_test = np.sqrt(mean_squared_error(y_test,pred_test_lr))
print('RSME test= ', rmse_test)

plt.scatter(y_train,pred_train_lr, label='Training Set')
plt.scatter(y_test,pred_test_lr, label='Test Set')

plt.xlabel('Real')
plt.ylabel('Predicted')
plt.legend()
plt.show()

coefficients = pd.DataFrame(multiple_linear_regression.coef_,X.columns.tolist())
coefficients.columns = ['Coefficients']
print(coefficients)
coefficients.plot.bar()

from sklearn.metrics import mean_squared_error, r2_score

y_pred = multiple_linear_regression.predict(X_test)

rmse_MLR = np.sqrt(mean_squared_error(y_test, y_pred))

r2 = r2_score(y_test, y_pred)

print('R2 test = ', r2)
print('RSME test = ', rmse_MLR)

coefficients = pd.DataFrame(multiple_linear_regression.coef_,X.columns.tolist())
coefficients.columns = ['Coefficients']
print(coefficients)

coefficients.plot.bar()

from sklearn.linear_model import Lasso

lasso_regression = Lasso(alpha=0.0001)
lasso_regression.fit(X = X_train, y = y_train)

from sklearn.metrics import r2_score, mean_squared_error
y_pred = lasso_regression.predict(X_test)


r2 = r2_score(y_test, y_pred)
print('R2 test = ', r2)

coeffecients = pd.DataFrame(lasso_regression.coef_,X.columns.tolist())
coeffecients.columns = ['Coeffecient']
print(coeffecients)

coeffecients.plot.bar()



X = df.drop('Hydrogen desorption (wt%)',axis=1)
Y = df['Hydrogen desorption (wt%)']
# Select only numerical columns (excluding 'object' type)
X = X.select_dtypes(exclude=['object'])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name} Mean Squared Error: {mse:.3f}, R² Score: {r2:.3f}, MAE: {mae:.3f}")
    print(' ')

# Initialize the StandardScaler
scaler = StandardScaler()

# Scale the training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train_scaled, y_train)  # Train with scaled data
    y_pred = model.predict(X_test_scaled)  # Predict with scaled test data
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name} Mean Squared Error: {mse:.3f}, R² Score: {r2:.3f}, MAE: {mae:.3f}")
    print(' ')

x = df_encoded.drop('Hydrogen desorption (wt%)',axis=1)
y = df_encoded['Hydrogen desorption (wt%)']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
selected_models = []
for name, model in models.items():
    model.fit(X_train, y_train)  # Train with scaled data
    y_pred = model.predict(X_test)  # Predict with scaled test data
    r2 = r2_score(y_test, y_pred)
    if r2 > 0.90:
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        print(f"{name} R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
        print(' ')
        selected_models.append((name, model))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

 # Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Cross-validation for each model
selected_models = []
for name, model in models.items():
    cv_scores = cross_val_score(model, x, y, cv=10, scoring='r2')  # Perform 5-fold cross-validation
    mean_r2 = cv_scores.mean()

    print(cv_scores,model)
    model.fit(X_train, y_train)  # Train the model on the entire training set
    y_pred = model.predict(X_test)  # Predict on the test set
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name} Mean R² Score: {mean_r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
    print(' ')
    selected_models.append((name, model))

# Split the dataset into training and testing sets
X = df.drop('Hydrogen desorption (wt%)',axis=1)
Y = df['Hydrogen desorption (wt%)']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

# Initialize the StandardScaler for numerical columns and OneHotEncoder for categorical columns
numerical_features = X_train.select_dtypes(exclude=['object']).columns
categorical_features = ['Material name']  # Replace 'Location' with your categorical column name
encoder = OneHotEncoder()

# Feature transformation function for log, square root, and polynomial features
log_sqrt_transformer = FunctionTransformer(np.log1p, validate=True)
polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

# Create a ColumnTransformer to handle preprocessing for both numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
     ('log_sqrt',log_sqrt_transformer,numerical_features),
       ('poly_trans',polynomial_transformer,numerical_features),
        ('cat', encoder, categorical_features)
    ]
)
# Initialize models
models = {
   'SGD': SGDRegressor(),
  'Random Forest': RandomForestRegressor(),
   'Gradient Boosting': GradientBoostingRegressor(),
   'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
   'Extra Trees': ExtraTreesRegressor()
}


# Train and evaluate each model
for name, model in models.items():
    # Use Pipeline to chain preprocessing and modeling steps
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    model_pipeline.fit(X_train, y_train)  # Train the model

def plot_correlation(data,target_column='Hydrogen desorption (wt%)'):

  df = pd.get_dummies(data, columns=['Material name'])
  x = df.drop(target_column, axis=1)
  cols = x.columns
  """
  Function that shows the correlation and clustering between the features:

  """
  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))
  corr = spearmanr(x).correlation

  # Ensure the correlation matrix is symmetric
  corr = (corr + corr.T) / 2
  np.fill_diagonal(corr, 1)
  # We convert the correlation matrix to a distance matrix before performing
  # hierarchical clustering using Ward's linkage.
  distance_matrix = 1 - np.abs(corr)
  dist_linkage = hierarchy.ward(squareform(distance_matrix))
  dendro = hierarchy.dendrogram(
      dist_linkage, labels=cols, ax=ax1, leaf_rotation=90
  )
  dendro_idx = np.arange(0, len(dendro["ivl"]))

  ax2.imshow(corr[dendro["leaves"], :][:, dendro["leaves"]])
  ax2.set_xticks(dendro_idx)
  ax2.set_yticks(dendro_idx)
  ax2.set_xticklabels(dendro["ivl"], rotation="vertical")
  ax2.set_yticklabels(dendro["ivl"])
  fig.tight_layout()
  plt.show()

plot_correlation(df)

# Split the dataset into training and testing sets
X = df
y = df['Hydrogen desorption (wt%)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing for numerical and categorical features
categorical_pipeline = OneHotEncoder(handle_unknown='ignore')  # One-hot encode categorical features

# Feature transformation function for log, square root, and polynomial features
log_sqrt_transformer = FunctionTransformer(np.log1p, validate=True)
polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

# Combine preprocessed numerical and categorical features
preprocessor = ColumnTransformer([
 ('log_sqrt',log_sqrt_transformer,numerical_features),
 ('poly_trans',polynomial_transformer,numerical_features),
 ('cat', categorical_pipeline, categorical_features),
])

# Models
model1 = LinearRegression()
model2 = XGBRegressor()
model3 = GradientBoostingRegressor()

# Stacking Regressor
stacking_model = StackingRegressor(
    estimators=[
        ('lr', model1),
        ('xgb', model2),
        ('gb', model3),
    ],
    final_estimator=GradientBoostingRegressor()# ExtraTreesRegressor() # Meta-model used to combine the base estimators' predictions
)
model_steps = [

        ('preprocessor',preprocessor),
        ('model', stacking_model)
    ]

# Construct the pipeline
model_pipeline = Pipeline(steps=model_steps)

# Train and evaluate the model
model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = model_pipeline.score(X_test, y_test)
print(f"R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")

###
joblib.dump(model_pipeline,"best_model.joblib")
# Production environment
best_model = joblib.load('best_model.joblib')
best_model.predict(X_train.iloc[:6])
