# -*- coding: utf-8 -*-
"""Anh_Week3_edited

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Urs_-_fZR4UYnNb6m3prfdkhJKswZL9a
"""

import pandas as pd
from google.colab import files

#upload the file
uploaded = files.upload()

#load the dataset
file_name = "SAF Dataset.xlsx"
df = pd.read_excel(file_name)

#display the first few rows
df.head()

# print the dataset's columns
print(df.columns)

# check the data types to identify numeric columns
df.dtypes

# check for missing data
missing_data = df.isnull().sum()

# display columns with missing data
missing_data[missing_data > 0]

# There is no missing data

import pandas as pd
import matplotlib.pyplot as plt

df['C (%)'].plot(kind='box')
plt.title('C (%)')
plt.ylabel('Values')
plt.show()
# There is no outlier in C

df['H (%)'].plot(kind='box')
plt.title('H (%)')
plt.ylabel('Values')
plt.show()

df['N (%)'].plot(kind='box')
plt.title('N (%)')
plt.ylabel('Values')
plt.show()

df['O (%)'].plot(kind='box')
plt.title('O (%)')
plt.ylabel('Values')
plt.show()

df['S (%)'].plot(kind='box')
plt.title('S (%)')
plt.ylabel('Values')
plt.show()

df['VM (%)'].plot(kind='box')
plt.title('VM (%)')
plt.ylabel('Values')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Calculate the IQR for each numerical column
outlier_threshold = 1.5
q1 = df.select_dtypes(include=np.number).quantile(0.25) # Select numerical columns only
q3 = df.select_dtypes(include=np.number).quantile(0.75) # Select numerical columns only
iqr = q3 - q1
lower_bound = q1 - outlier_threshold * iqr
upper_bound = q3 + outlier_threshold * iqr

# Filter out outliers
df_without_outliers = df[(df[df.select_dtypes(include=np.number).columns] >= lower_bound) & (df[df.select_dtypes(include=np.number).columns] <= upper_bound)] # Use df instead of filtered_df

# Create the box plot without outliers
df_without_outliers.boxplot()
plt.title('Box Plots for Numerical Columns (without outliers)')
plt.xticks(rotation=45)
plt.show()

# Outliers are handled by calculating IQR and then removing using lower and upper bound

# Summarize key characteristics of the data without outliers
summary_stats = df_without_outliers.describe().T
summary_stats['IQR'] = summary_stats['75%'] - summary_stats['25%']
summary_stats = summary_stats[['mean', '50%', 'std', 'min', 'max', 'IQR']]
summary_stats.columns = ['Mean', 'Median', 'Standard Deviation', 'Minimum', 'Maximum', 'IQR']

print(summary_stats)

# C, VM, and O have higher values
# FC, Cel, Ash, Hem, ad Plant Capacity have high standard deviation

# Identify numeric columns
numeric_columns = df_without_outliers.select_dtypes(include=['float', 'int']).columns

# Identify categorical columns (assuming they are of type 'object')
categorical_columns = df_without_outliers.select_dtypes(include=['object']).columns

df['Plant capacity (kg/hr)'] = df['Plant capacity (kg/hr)'].astype(float)
print(df.dtypes)

# Apply one-hot-encoding
!pip install scikit-learn # install the scikit-learn module that contains the OneHotEncoder object
from sklearn.preprocessing import OneHotEncoder # import the OneHotEncoder object
encoder = OneHotEncoder(sparse=False, drop='first')
encoded_data = encoder.fit_transform(df[categorical_columns])

# Convert the encoded data to a Pandas DataFrame
encoded_df = pd.DataFrame(encoded_data)

def get_duplicate_columns(df_encoded):
    duplicate_column_names = set()
    for i in range(df_encoded.shape[1]):
        col = df_encoded.iloc[:, i]
        for j in range(i + 1, df_encoded.shape[1]):
            other_col = df_encoded.iloc[:, j]
            if col.equals(other_col):
                duplicate_column_names.add(df_encoded.columns[j])
    return list(duplicate_column_names)

# Find duplicate columns
duplicate_columns = get_duplicate_columns(encoded_df) # Pass the DataFrame to the function
print("Duplicate columns:", duplicate_columns)

# No duplicates

import seaborn as sns
import matplotlib.pyplot as plt

# Identify non-numeric columns in df_without_outliers
non_numeric_columns_outliers = df_without_outliers.select_dtypes(include=['object']).columns
print("Non-numeric columns in df_without_outliers:", non_numeric_columns_outliers)

# Remove non-numeric columns
df_numeric_outliers = df_without_outliers.drop(columns=non_numeric_columns_outliers)

# Compute correlation matrix for df_without_outliers
corr_matrix_outliers = df_numeric_outliers.corr()
print(corr_matrix_outliers)

# Plot correlation matrix heatmap for df_without_outliers
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix_outliers, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix Heatmap for df_without_outliers')
plt.show()

# Variables such as Ash (%) and N (%) show a strong positive correlation (~0.69). This means that as nitrogen content increases, the ash content tends to increase as well.
# Cellulose (Cel) and Hemicellulose (Hem) show a perfect correlation of 1.0, indicating a strong relationship between them.
# MSP and Plant capacity (kg/hr) have a strong positive correlation (~0.6), implying that as the plant capacity increases, MSP also increases.

print(df.columns.tolist())

print(df_without_outliers.columns.tolist())

# Verify column names to ensure proper naming
print(df.columns.tolist())

# Use the correct column names for plotting
if 'C (%)' in df_without_outliers.columns and 'H (%)' in df_without_outliers.columns:
    sns.scatterplot(x='C (%)', y='H (%)', data=df_without_outliers)
    plt.title('Scatter plot of C (%) vs H (%)')
    plt.show()
else:
    print("C (%) or H (%) column not found")

# Bar plot for Location (categorical variable)
if 'Location' in df_without_outliers.columns:
    sns.countplot(x='Location', data=df)
    plt.title('Count plot of Location')
    plt.show()
else:
    print("Location column not found")

# Pair plot for multiple numeric columns
numeric_columns = ['C (%)', 'H (%)', 'N (%)', 'O (%)', 'S (%)', 'VM (%)', 'Ash (%)', 'FC (%)', 'Cel (%)', 'Hem (%)', 'Lig (%)', 'Plant capacity (kg/hr)', 'MSP']

# Check if the numeric columns exist in the DataFrame
numeric_columns = [col for col in numeric_columns if col in df_without_outliers.columns]

if len(numeric_columns) > 1:
    sns.pairplot(df_without_outliers[numeric_columns].dropna())
    plt.show()
else:
    print("Insufficient numeric columns for pair plot.")

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Select only numeric columns from the DataFrame
numeric_df = df_without_outliers.select_dtypes(include=[float, int])

# Calculate the correlation matrix
corr_matrix = numeric_df.corr()

# Print the correlation matrix
print("Correlation Matrix:\n", corr_matrix)

# Identify pairs of highly correlated variables
threshold = 0.9
high_corr_pairs = []

for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))

print("\nHighly Correlated Pairs (threshold = 0.9):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]}: {pair[2]:.2f}")

#Combining highly correlated variables
# Standardize the data
numeric_df = df_without_outliers.select_dtypes(include=[float, int]) # Use df_without_outliers instead of df
scaler = StandardScaler()

# Drop rows with NaN values before scaling
numeric_df = numeric_df.dropna() # Added line to drop rows with NaN values
scaled_data = scaler.fit_transform(numeric_df)

# Apply PCA to reduce multicollinearity
pca = PCA(n_components=len(numeric_df.columns))
pca.fit(scaled_data)

# Check explained variance to decide how many components to keep
explained_variance = pca.explained_variance_ratio_
print("Explained variance by each principal component:", explained_variance)

cumulative_variance = explained_variance.cumsum()
n_components_to_keep = next(i for i, total_var in enumerate(cumulative_variance) if total_var > 0.95) + 1

# Transform data using the chosen number of components
df_pca = pca.transform(scaled_data)[:, :n_components_to_keep]

print(f"Reduced data using {n_components_to_keep} principal components.")

# After completing the Exploratory Data Analysis (EDA), only variables C and O show strong correlations.
# In relation to the MSP of SAF, there doesn't appear to be significant correlation with any of the variables.
# Outliers were addressed by removing the affected data points.
