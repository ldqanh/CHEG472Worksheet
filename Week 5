# -*- coding: utf-8 -*-
"""ML Workshop

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lVbsfkiOT5HDpD9nbUSDSfGSimphG0vK
"""

!pip install shap -q


import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, train_test_split


from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import PolynomialFeatures
import numpy as np

from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import SGDRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import ExtraTreesRegressor
import joblib
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt
import matplotlib as mpl
import scipy.stats as stats
import plotly.graph_objects as go
import plotly.express as px
from scipy.stats import spearmanr
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform

# Import filters to remove unnecessary warnings
from warnings import simplefilter
import warnings
warnings.filterwarnings("ignore")
from sklearn.exceptions import ConvergenceWarning

from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform

# Import filters to remove unnecessary warnings
from warnings import simplefilter
import warnings
warnings.filterwarnings("ignore")

from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error
import shap

import pandas as pd
from google.colab import files

#upload the file
uploaded = files.upload()

#load the dataset
file_name = "SAF Dataset (1).xlsx"
df = pd.read_excel(file_name)

#display the first few rows
df.head()
# check if the data contains null values
df.isna().sum()

# Check for duplicates in the entire dataset
duplicates = df.duplicated()
# If there are any duplicates, the 'duplicates' variable will contain True for those rows
if duplicates.any():
    # Get the rows with duplicates
    duplicate_rows = df[duplicates]
else:
    print("No duplicates found in the dataset.")

duplicate_rows

# drop duplicates
data = df.drop_duplicates()

data.describe()  # Summary statistics for numerical columns

data.dtypes

data['Location '].unique()

data['Location '].value_counts() # number of occurence of each unique value

sns.pairplot(data)

numerical_columns = data.select_dtypes(include=['float64', 'int64'])  # Selecting only numerical columns
correlations = numerical_columns.corr()['MSP']  # Computing the correlation matrix
print(correlations)

from sklearn.model_selection import train_test_split

X = data[['Plant capacity (kg/hr)']]
y = data['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)#### Data Cleansing
##Data cleansing, also known as data cleaning or data scrubbing, is the process of identifying and correcting errors, inconsistencies, inaccuracies, and incomplete data in a dataset. The goal of data cleansing is to improve the quality of the data, making it more accurate, reliable, and suitable for analysis. Clean data is essential for making informed decisions, conducting meaningful analyses, and building reliable machine learning models.
##Here are common steps involved in data cleansing:
from sklearn.model_selection import train_test_split

X = data[['Plant capacity (kg/hr)']]
y = data['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.linear_model import LinearRegression

linear_regression = LinearRegression()
linear_regression.fit(X = X_train, y = y_train)

#β1 slope and β0 is the y-intercept for linear regression

from sklearn.metrics import r2_score
y_pred_test = linear_regression.predict(X_test)
y_pred_train = linear_regression.predict(X_train)

print('R2 train = ', r2_score(y_train, y_pred_train))
print('R2 test = ', r2_score(y_test, y_pred_test))

plt.scatter(y_train,y_pred_train, label='Training Set')
plt.scatter(y_test,y_pred_test, label='Test Set')
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split

X = df.drop(['Location ', 'MSP'], axis=1)
y = df['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

from sklearn.linear_model import LinearRegression

multiple_linear_regression = LinearRegression()
multiple_linear_regression.fit(X = X_train, y = y_train)

from sklearn.metrics import r2_score

pred_train_lr = multiple_linear_regression.predict(X_train)
pred_test_lr = multiple_linear_regression.predict(X_test)

print('R2 training = ', r2_score(y_train, pred_train_lr))
print('R2 test = ', r2_score(y_test, pred_test_lr))

from sklearn.metrics import mean_squared_error
rmse_test = np.sqrt(mean_squared_error(y_test,pred_test_lr))
print('RSME test= ', rmse_test)

plt.scatter(y_train,pred_train_lr, label='Training Set')
plt.scatter(y_test,pred_test_lr, label='Test Set')

plt.xlabel('Real')
plt.ylabel('Predicted')
plt.legend()
plt.show()

coefficients = pd.DataFrame(multiple_linear_regression.coef_,X.columns.tolist())
coefficients.columns = ['Coefficients']
print(coefficients)

coefficients.plot.bar()

from sklearn.model_selection import train_test_split

X = df.drop(['Location ', 'MSP'], axis=1)
y = df['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn import preprocessing

scaler = preprocessing.StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.linear_model import LinearRegression

multiple_linear_regression = LinearRegression()
multiple_linear_regression.fit(X = X_train, y = y_train)

from sklearn.metrics import mean_squared_error, r2_score

y_pred = multiple_linear_regression.predict(X_test)

rmse_MLR = np.sqrt(mean_squared_error(y_test, y_pred))

r2 = r2_score(y_test, y_pred)

print('R2 test = ', r2)
print('RSME test = ', rmse_MLR)

coefficients = pd.DataFrame(multiple_linear_regression.coef_,X.columns.tolist())
coefficients.columns = ['Coefficients']
print(coefficients)

coefficients.plot.bar()

from sklearn.linear_model import Lasso

lasso_regression = Lasso(alpha=0.0001)
lasso_regression.fit(X = X_train, y = y_train)

from sklearn.metrics import r2_score, mean_squared_error
y_pred = lasso_regression.predict(X_test)


r2 = r2_score(y_test, y_pred)
print('R2 test = ', r2)

coeffecients = pd.DataFrame(lasso_regression.coef_,X.columns.tolist())
coeffecients.columns = ['Coeffecient']
print(coeffecients)

coeffecients.plot.bar()

X = data.drop('MSP',axis=1)
Y = data['MSP']
# Select only numerical columns (excluding 'object' type)
X = X.select_dtypes(exclude=['object'])

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name} Mean Squared Error: {mse:.3f}, R² Score: {r2:.3f}, MAE: {mae:.3f}")
    print(' ')

# Initialize the StandardScaler
scaler = StandardScaler()

# Scale the training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
for name, model in models.items():
    model.fit(X_train_scaled, y_train)  # Train with scaled data
    y_pred = model.predict(X_test_scaled)  # Predict with scaled test data
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    print(f"{name} Mean Squared Error: {mse:.3f}, R² Score: {r2:.3f}, MAE: {mae:.3f}")
    print(' ')

# Apply one-hot encoding to specific columns
encoded_df = pd.get_dummies(data, columns=['Location '])
x = encoded_df.drop('MSP',axis=1)
y = encoded_df['MSP']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
selected_models = []
for name, model in models.items():
    model.fit(X_train, y_train)  # Train with scaled data
    y_pred = model.predict(X_test)  # Predict with scaled test data
    r2 = r2_score(y_test, y_pred)
    if r2 > 0.90:
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        print(f"{name} R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
        print(' ')
        selected_models.append((name, model))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

 # Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Cross-validation for each model
selected_models = []
for name, model in models.items():
    cv_scores = cross_val_score(model, x, y, cv=10, scoring='r2')  # Perform 5-fold cross-validation
    mean_r2 = cv_scores.mean()
    if mean_r2 > 0.2:
        print(cv_scores,model)
        model.fit(X_train, y_train)  # Train the model on the entire training set
        y_pred = model.predict(X_test)  # Predict on the test set
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        print(f"{name} Mean R² Score: {mean_r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
        print(' ')
        selected_models.append((name, model))

# Split the dataset into training and testing sets
X = data.drop('MSP',axis=1)
Y = data['MSP']
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

# Initialize the StandardScaler for numerical columns and OneHotEncoder for categorical columns
numerical_features = X_train.select_dtypes(exclude=['object']).columns
categorical_features = ['Location ']  # Replace 'Location' with your categorical column name
encoder = OneHotEncoder()

# Feature transformation function for log, square root, and polynomial features
log_sqrt_transformer = FunctionTransformer(np.log1p, validate=True)
polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

# Create a ColumnTransformer to handle preprocessing for both numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
     ('log_sqrt',log_sqrt_transformer,numerical_features),
       ('poly_trans',polynomial_transformer,numerical_features),
        ('cat', encoder, categorical_features)
    ]
)
# Initialize models
models = {
   'SGD': SGDRegressor(),
  'Random Forest': RandomForestRegressor(),
   'Gradient Boosting': GradientBoostingRegressor(),
   'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
   'Extra Trees': ExtraTreesRegressor()
}


# Train and evaluate each model
for name, model in models.items():
    # Use Pipeline to chain preprocessing and modeling steps
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    model_pipeline.fit(X_train, y_train)  # Train the model

    # No need to separately encode X_test, the pipeline handles it
    r2 = model_pipeline.score(X_test, y_test)  # R² Score
    if r2 > 0.95:
        y_pred = model_pipeline.predict(X_test)  # Predict
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        print(f"{name} R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")

def plot_correlation(data,target_column='MSP'):

  df = pd.get_dummies(data, columns=['Location '])
  x = df.drop(target_column, axis=1)
  cols = x.columns
  """
  Function that shows the correlation and clustering between the features:

  """
  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 6))
  corr = spearmanr(x).correlation

  # Ensure the correlation matrix is symmetric
  corr = (corr + corr.T) / 2
  np.fill_diagonal(corr, 1)
  # We convert the correlation matrix to a distance matrix before performing
  # hierarchical clustering using Ward's linkage.
  distance_matrix = 1 - np.abs(corr)
  dist_linkage = hierarchy.ward(squareform(distance_matrix))
  dendro = hierarchy.dendrogram(
      dist_linkage, labels=cols, ax=ax1, leaf_rotation=90
  )
  dendro_idx = np.arange(0, len(dendro["ivl"]))

  ax2.imshow(corr[dendro["leaves"], :][:, dendro["leaves"]])
  ax2.set_xticks(dendro_idx)
  ax2.set_yticks(dendro_idx)
  ax2.set_xticklabels(dendro["ivl"], rotation="vertical")
  ax2.set_yticklabels(dendro["ivl"])
  fig.tight_layout()
  plt.show()

plot_correlation(data)

# feature selection
cols_ = [
    'H (%)', 'N (%)', 'O (%)', 'VM (%)', 'Ash (%)', 'Cel (%)', 'Hem (%)',
    'Plant capacity (kg/hr)', 'Location '
]

X = data.drop('MSP', axis=1)
X = X[cols_]
Y = data['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)

# Initialize the StandardScaler for numerical columns and OneHotEncoder for categorical columns
numerical_features = X_train.select_dtypes(exclude=['object']).columns
categorical_features = ['Location ']  # Replace 'Location' with your categorical column name
encoder = OneHotEncoder()

# Feature transformation function for log and polynomial features
log_transformer = FunctionTransformer(np.log1p, validate=True)
polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

# Create a ColumnTransformer to handle preprocessing for both numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', encoder, categorical_features),
        ('log', log_transformer, numerical_features),  # Apply log transform to numerical columns
        ('poly', polynomial_transformer, numerical_features)  # Apply polynomial features to numerical columns
    ]
)

# Initialize models
models = {
    'SGD': SGDRegressor(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'AdaBoost': AdaBoostRegressor(),
    'Decision Tree': DecisionTreeRegressor(),
    'MLP': MLPRegressor(),
    'SVR': SVR(),
    'XGBoost': XGBRegressor(),
    'Linear Regression': LinearRegression(),
    'Extra Trees': ExtraTreesRegressor()
}

# Train and evaluate each model
selected_models = []
for name, model in models.items():
    # Use Pipeline to chain preprocessing and modeling steps
    model_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', model)
    ])

    model_pipeline.fit(X_train, y_train)  # Train the model

    # No need to separately encode X_test, the pipeline handles it
    r2 = model_pipeline.score(X_test, y_test)  # R² Score
    if r2 > 0.95:
        y_pred = model_pipeline.predict(X_test)  # Predict
        mse = mean_squared_error(y_test, y_pred)
        mae = mean_absolute_error(y_test, y_pred)
        print(f"{name} R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
        selected_models.append((name, model))


# XGBoost  improved as well in addition to the previous models

# Define your columns for different types of features
numerical_features = ['H (%)', 'N (%)', 'O (%)', 'VM (%)', 'Ash (%)', 'Cel (%)', 'Hem (%)', 'Plant capacity (kg/hr)']
categorical_features = ['Location ']

# Split the dataset into training and testing sets
X = data[numerical_features + categorical_features]
y = data['MSP']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing for numerical and categorical features
categorical_pipeline = OneHotEncoder(handle_unknown='ignore')  # One-hot encode categorical features

# Feature transformation function for log, square root, and polynomial features
log_sqrt_transformer = FunctionTransformer(np.log1p, validate=True)
polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

# Combine preprocessed numerical and categorical features
preprocessor = ColumnTransformer([
 ('log_sqrt',log_sqrt_transformer,numerical_features),
 ('poly_trans',polynomial_transformer,numerical_features),
 ('cat', categorical_pipeline, categorical_features),
])

# Models
model1 = LinearRegression()
model2 = XGBRegressor()
model3 = GradientBoostingRegressor()

# Stacking Regressor
stacking_model = StackingRegressor(
    estimators=[
        ('lr', model1),
        ('xgb', model2),
        ('gb', model3),
    ],
    final_estimator=GradientBoostingRegressor()# ExtraTreesRegressor() # Meta-model used to combine the base estimators' predictions
)
model_steps = [

        ('preprocessor',preprocessor),
        ('model', stacking_model)
    ]

# Construct the pipeline
model_pipeline = Pipeline(steps=model_steps)

# Train and evaluate the model
model_pipeline.fit(X_train, y_train)
y_pred = model_pipeline.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = model_pipeline.score(X_test, y_test)
print(f"R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")

###
joblib.dump(model_pipeline,"best_model.joblib")
# Production environment
best_model = joblib.load('best_model.joblib')
best_model.predict(X_train.iloc[:6])

def plot_feature_importance(data, target_column, model):
    encoded_df = pd.get_dummies(data, columns=['Location '])
    x = encoded_df.drop(target_column, axis=1)
    y = encoded_df[target_column]

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

    # Train and evaluate the model
    model.fit(X_train, y_train)  # Train with scaled data
    # Feature Importance plot
    feature_importance = model.feature_importances_
    sorted_idx = feature_importance.argsort()

    fig = go.Figure(go.Bar(
        x=feature_importance[sorted_idx],
        y=x.columns[sorted_idx],
        orientation='h'
    ))
    fig.update_layout(title="Extra Trees Regressor - Feature Importance")
    fig.update_layout(height=600, width=1000, title_text="Feature Importance and SHAP Summary")
    fig.show()

def plot_shap(data, target_column, model):
    encoded_df = pd.get_dummies(data, columns=['Location '])
    x = encoded_df.drop(target_column, axis=1)
    y = encoded_df[target_column]

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

    # Train and evaluate the model
    model.fit(X_train, y_train)  # Train with scaled data
    # SHAP summary plot
    explainer = shap.Explainer(model)
    shap_values = explainer(X_train)
    shap.summary_plot(shap_values, X_train,
                    plot_size= (10,7))



def plot_local_shap(data, target_column, model,id):
   # from shap import Explainer, Explanation
   # from shap import waterfall_plot

    encoded_df = pd.get_dummies(data, columns=['Location '])
    x = encoded_df.drop(target_column, axis=1)
    y = encoded_df[target_column]

    # Split the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=42)

    # Train the model
    model.fit(X_train, y_train)  # Train with scaled data
    # Fits the explainer
    explainer = shap.Explainer(model)
    # Calculates the SHAP values - It takes some time
    shap_values = explainer(X_train)
    #shap_values = explainer.shap_values(X_train)
    shap.plots.waterfall(shap_values[id])

def plot_scatter(y_train, pred_train, y_test, pred_test):
    # pred_train_ = model.predict(X_train)
    #pred_test = model.predict(X_test)
    # Data for the scatter plot
    trace1 = go.Scatter(x=y_train, y=pred_train, mode='markers', name='Training Set',
                        marker=dict(color='blue', size=8))
    trace2 = go.Scatter(x=y_test, y=pred_test, mode='markers', name='Test Set',
                        marker=dict(color='orange', size=8))

    # Plot layout
    layout = go.Layout(
        xaxis=dict(title='Real'),
        yaxis=dict(title='Predicted'),
        legend=dict(x=0, y=1.0, bgcolor='rgba(255, 255, 255, 0)')
    )
     # Create the figure and plot
       # Create the figure and plot
    fig = go.Figure(data=[trace1, trace2], layout=layout)
    fig.update_layout(plot_bgcolor="#FFFFFF",width=1000,height=600,showlegend=True,  title={
            'text': 'Scatter Plot for Predicted and Actual ',
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        },)
    fig.show()

def train_models(data, target_column='MSP'):
    # Split the dataset into training and testing sets
    cols_ = [
        'H (%)', 'N (%)', 'O (%)', 'VM (%)', 'Ash (%)', 'Cel (%)', 'Hem (%)',
        'Plant capacity (kg/hr)', 'Location '
    ]

    X = data.drop(target_column, axis=1)
    Y = data[target_column]

    X_train, X_test, y_train, y_test = train_test_split(X[cols_], Y, test_size=0.20, random_state=42)

    # Initialize the StandardScaler for numerical columns and OneHotEncoder for categorical columns
    numerical_features = X_train.select_dtypes(exclude=['object']).columns
    categorical_features = ['Location ']  # Replace 'Location' with your categorical column name

    encoder = OneHotEncoder()

    # Feature transformation function for log and polynomial features
    log_transformer = FunctionTransformer(np.log1p, validate=True)
    polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

    # Create a ColumnTransformer to handle preprocessing for both numerical and categorical features
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', encoder, categorical_features),
            ('log', log_transformer, numerical_features),  # Apply log transform to numerical columns
            ('poly', polynomial_transformer, numerical_features)  # Apply polynomial features to numerical columns
        ]
    )

    # Initialize models
    models = {
        'SGD': SGDRegressor(),
        'Random Forest': RandomForestRegressor(),
        'Gradient Boosting': GradientBoostingRegressor(),
        'AdaBoost': AdaBoostRegressor(),
        'Decision Tree': DecisionTreeRegressor(),
        'MLP': MLPRegressor(),
        'SVR': SVR(),
        'XGBoost': XGBRegressor(),
        'Linear Regression': LinearRegression(),
        'Extra Trees': ExtraTreesRegressor()
    }

    # Train and evaluate each model
    selected_models = []
    for name, model in models.items():
        # Use Pipeline to chain preprocessing and modeling steps
        model_pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('model', model)
        ])

        model_pipeline.fit(X_train, y_train)  # Train the model

        # No need to separately encode X_test, the pipeline handles it
        r2 = model_pipeline.score(X_test, y_test)  # R² Score
        if r2 > 0.95:
            y_pred = model_pipeline.predict(X_test)  # Predict
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            print(f"{name} R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")
            print(' ')
            selected_models.append((name, model))

    return

def train_evaluate_stacking_model(data):
    # Define your columns for different types of features
    numerical_features = ['H (%)', 'N (%)', 'O (%)', 'VM (%)', 'Ash (%)', 'Cel (%)', 'Hem (%)', 'Plant capacity (kg/hr)'] # after feature engineering
    categorical_features = ['Location ']

    # Split the dataset into training and testing sets
    X = data[numerical_features + categorical_features]
    y = data['MSP']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Preprocessing for numerical and categorical features
    categorical_pipeline = OneHotEncoder(handle_unknown='ignore')  # One-hot encode categorical features

    # Feature transformation function for log, square root, and polynomial features
    log_sqrt_transformer = FunctionTransformer(np.log1p, validate=True)
    polynomial_transformer = PolynomialFeatures(degree=2, include_bias=False)

    # Combine preprocessed numerical and categorical features
    preprocessor = ColumnTransformer([
        ('log_sqrt', log_sqrt_transformer, numerical_features),
        ('poly_trans', polynomial_transformer, numerical_features),
        ('cat', categorical_pipeline, categorical_features),
    ])

    # Models
    model1 = LinearRegression()
    model2 = ExtraTreesRegressor()
    model3 = GradientBoostingRegressor()

    # Stacking Regressor
    stacking_model = StackingRegressor(
        estimators=[
            ('lr', model1),
            ('rf', model2),
            ('gb', model3)
        ],
        final_estimator=ExtraTreesRegressor()  # Meta-model used to combine the base estimators' predictions
    )

    model_steps = [
        ('preprocessor', preprocessor),
        ('model', stacking_model)
    ]

    # Construct the pipeline
    model_pipeline = Pipeline(steps=model_steps)

    # Train and evaluate the model
    model_pipeline.fit(X_train, y_train)
    y_pred = model_pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = model_pipeline.score(X_test, y_test)
    print(f"R² Score: {r2:.3f}, Mean Squared Error: {mse:.3f}, MAE: {mae:.3f}")


#train_models(data)

#plot_feature_importance(data, 'MSP', model)

# Example usage with your dataset
train_evaluate_stacking_model(data)

# plot feature importance for a GBR model
model = GradientBoostingRegressor()
plot_feature_importance(data,'MSP',model)

# define model to use
model = GradientBoostingRegressor()
plot_shap(data,'MSP',model)

plot_local_shap(data, 'MSP', model,3)

